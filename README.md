# Bigram을 활용한 연어 학습 프로그램

<h3> 단어장 학습 어플 디자인 </h3>
<link> https://www.figma.com/design/RJUVsOzDOj33UpYwPyYpdC/Untitled?node-id=21-2&t=A4Fc0BGLwGOkPuSM-1 </link>

<h3>1. 주제 및 선정 동기</h3>
  언어 학습에 있어 어휘 지식은 필수적이다. 어휘를 알아야 언어를 이해하고 더 나아가 직접 활용할 수 있다. 이러한 어휘의 중요성에 집중하여 다수의 학생들이 영단어의 뜻을 하나씩 암기하곤 한다. 그러나 이 방식은 암기한 지식이 단기 기억에만 남게 되어 머지않아 잊게 된다. 이에 영어교육 학계에서는 ‘학습한 어휘 지식을 어떻게 효과적으로 체화할 수 있을까?’에 대한 연구가 꾸준히 이어져 왔다. 연구자들은 학습자가 특정 지식을 학습할 때 그것을 독립적으로 기억에 저장하는 것이 아니라 관련이 있는 다른 기억들과 연결을 지어 습득하게 된다는 것에 집중하였다. 단어들은 독립적으로 의미를 갖기보단 맥락 속에 존재하기에 온전히 습득하기 위해서는 ‘의미있는 학습’이 이루어져야 한다. 다시 말해, 학생이 어휘를 학습할 때 새로운 지식을 독립적으로 기억하는 것이 아닌 기존에 있던 지식과의 연결을 지어 기억 속에 더 오래 남을 수 있도록 해야 한다. 특히나 어휘를 적절히 활용할 수 있게 되기 위해서는 단어가 사용되는 맥락이나 함께 사용되는 단어 등을 충분히 고려해야 하므로 연어(collocation) 학습의 중요성이 대두되었다. <br>
	어휘와 연어 교수 방법의 하나로 말뭉치 활용이 제시되었다. 수백억 개의 언어 자료에서 학습자가 단어나 어구를 검색하여 어휘 활용 용례나 맥락 등을 파악할 수 있다. 단어를 하나씩 암기하는 것보다 검색 결과를 보며 귀납적으로 어휘를 학습하여 더욱 효과적으로 단어를 학습할 수 있다. 하지만 말뭉치의 한계점은 과도하게 방대한 양의 자료를 포함하고 있어 우리나라 학생들의 영어 수준에 비해 난이도가 높은 자료들이 대부분이라는 점이다. 말뭉치 검색 결과를 보면 수십개의 문장들이 한 화면 안에 뜨기 때문에 가독성이 낮다. 또한 일일 단어 검색 한도가 정해져 있어 유료로 비용을 지불하지 않는 이상 사용하기에 매우 불편하다. 이러한 한계점을 극복한 연어 학습 프로그램을 만들어보고자 이 프로젝트를 기획하였다. <br>
말뭉치와 유사한 기능을 하나, 적은 자료로 정확한 결과를 얻기 위해 Bigram 방식을 활용하였다. Bigram에 대해 배우면서 두 단어가 동시에 출현하는 빈도수를 분석하여 단어 간 연관성을 알아내는 과정이 인간의 언어 학습 방식과 다르지 않다는 생각이 들었다. 연속적으로 등장하는 단어들로 이루어진 토큰들을 분석하여 함께 자주 사용되는 단어들을 파악할 수 있다. 이 결과물을 이용하여 학습자에게 특정 단어의 연어를 제시한다면 효과적인 어휘 교수 도구로서 활용할 수 있을 것이다. 


<h3>2. 접근 방향 및 방법</h3>
	말뭉치의 단점을 개선하기 위하여 우리나라 고등학교 학생들을 위한 프로그램을 제작하였다. 학습자의 수준에 맞추고자 Bigram에 활용할 언어 자료로 대학수학능력시험과 고등학교 3학년 모의 평가 및 학력 평가를 선택하였다. 시험 자료들이 모두 PDF 파일 형식이었기에 프로그램에 활용하기 위해 데이터 가공 과정이 필요했다. 우선 한PDF의 DOCX로 변환하기 기능으로 시험지 PDF를 모두 .docx로 변환하였다. 워드에서 모든 시험지를 병합하고 .txt 확장자로 저장하여 글자만 추출하였다. 다음으로 Google Colaboratory에서 텍스트 파일을 불러와서 re 모듈을 사용하여 알파벳, 문장부호, 선지의 번호만 남기고 한글을 모두 삭제하였다. 여기에서 데이터를 문장 단위로 끊어주기 위해 re의 split 메소드로 문장부호들과 선지 번호를 기준으로 나누어 리스트에 저장하였다. 이 과정에서 리스트에 공백인 요소가 포함되어 최종으로 사용할 데이터는 공백이 아닌 요소들만 저장하였다. <br>
	먼저 사용자로부터 검색할 단어를 입력받는다. 입력 받은 단어를 기반으로 데이터에서 단어를 포함하고 있는 문장들만 골라낸다. 필터링된 문장들을 단어마다 토큰화하고 bigrams 함수로 bigram token을 생성한다. Bigram token 중 사용자가 검색한 단어만 포함한 토큰들만 반환하도록 다시 필터링한다. 결과로 나온 단어가 어순 상 검색 단어의 앞에 오는 지 뒤에 오는 지 사용자가 알아야 하기 때문에 토큰들을 검색 단어 뒤에 오는 단어들과 앞에 오는 단어들로 분류하는 과정을 거친다. 앞 단어와 뒷 단어들을 리스트에 각각 나눈 후, 등장하는 빈도수가 가장 큰 토큰들을 결과로 보여준다. 여기서 빈도수를 가중치로 계산하여 노드의 크기로 활용하였다. <br>
	 결과로 나온 단어들을 학습자가 한 눈에 보기 쉽도록 networkx로 시각화하였다. 검색한 단어가 들어가는 중심 노드 주위로 결과 단어들을 포함하는 노드들이 연결되도록 작성하였다. 빈도수에 따라 노드의 크기를 설정하도록 하였으며, 중심노드와 이웃노드들의 색상을 달리하였다. 앞에 오는 단어와 뒤에 오는 연어를 같은 방식을 활용하여 그래프를 각각 그리도록 했다. 이 결과물이 정확히 어떤 맥락에서 사용되는지도 보여주기 위해 해당 단어들을 포함하는 예문들도 제시하였다. 


<h3>3. 결론</h3>
계획했던 프로그램을 구현하는 데 성공하였다. 그러나 단어를 검색할 때 단어의 어형 변화 등을 고려하지 못하여 play와 같은 단어를 검색하면 동사보다 명사로써의 play의 연어 결과의 비중이 훨씬 컸다. 게다가 연어로 자주 등장하는 단어를 찾기 위해 빈도수를 기준으로 결과를 출력했는데, 그저 어디에나 자주 쓰이는 to, can, a, the 등의 단어들 위주로 결과가 나온 듯했다. 즉, 정확하지만 연어 학습 용도에 있어 타당도는 다소 떨어지는 결과였다. 이러한 문제점을 개선할 수 있는 방안으로 nltk의 lemmatizer을 활용하여 문장마다 어근을 추출하고 stopwords로 불용어를 설정하는 방법이 있다. 이 방법으로 프로그램을 발전시키고자 했으나, 어근을 추출하고 다시 문장으로 합치는 과정에서 모든 단어들이 섞이게 되어 bigram 분석이 불가했다. 또한 불용어의 설정 기준도 어떤 단어를 빼고 포함시킬 것인지 모호해지고 설정해야 할 언어가 방대해져 실현하지 못했다. 
아래는 play를 검색한 결과이다.
 
 ![image](https://github.com/user-attachments/assets/b3cea340-1d81-4017-9382-b859d1c59eb9)
![image](https://github.com/user-attachments/assets/4414188f-9dda-4a2b-80ce-88d0920e3cd0)

![image](https://github.com/user-attachments/assets/1574aa83-eb81-43da-88eb-b044612ec738)
![image](https://github.com/user-attachments/assets/af1140bb-d74c-4429-b02b-991d70bccafe)

 
 
